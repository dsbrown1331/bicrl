Ground truth theta: 1.0
Lava is at [0.54889938 0.5104231 ]
The optimal policies start from:
0.0 : (0.11055743640291682, 0.5264020375236573)
0.2 : (0.5634979617953978, 0.19068918303021265)
0.4 : (0.9703146595603623, 0.9810315789315387)
0.6 : (0.40603694152001457, 0.5903479485914557)
0.8 : (0.6071895707747583, 0.3852113220224068)
1.0 : (0.19156605232326396, 0.1980820738844118)
This optimal policy starts from: (0.19156605232326396, 0.1980820738844118)
This demo is starting from 0.011130329601843902 0.30712296381020643
World 1 , 1 demos
[array([0.01113033, 0.30712296])]
BOUND: 0.048489625937646816


Good for bound 0.1; learned theta was 1.0
Map policy starts from (0.19156605232326396, 0.1980820738844118)
Good for bound 0.2; learned theta was 1.0
Map policy starts from (0.19156605232326396, 0.1980820738844118)
Good for bound 0.3; learned theta was 1.0
Map policy starts from (0.19156605232326396, 0.1980820738844118)
Good for bound 0.4; learned theta was 1.0
Map policy starts from (0.19156605232326396, 0.1980820738844118)
Good for bound 0.5; learned theta was 1.0
Map policy starts from (0.19156605232326396, 0.1980820738844118)
This demo is starting from 0.4522898251173443 0.10783231825589046
World 1 , 2 demos
[array([0.01113033, 0.30712296]), array([0.45228983, 0.10783232])]
BOUND: 0.048489625937646816


Good for bound 0.1; learned theta was 1.0
Map policy starts from (0.19156605232326396, 0.1980820738844118)
Good for bound 0.2; learned theta was 1.0
Map policy starts from (0.19156605232326396, 0.1980820738844118)
Good for bound 0.3; learned theta was 1.0
Map policy starts from (0.19156605232326396, 0.1980820738844118)
Good for bound 0.4; learned theta was 1.0
Map policy starts from (0.19156605232326396, 0.1980820738844118)
Good for bound 0.5; learned theta was 1.0
Map policy starts from (0.19156605232326396, 0.1980820738844118)
This demo is starting from 0.5965917490154998 0.3104203820987277
World 1 , 3 demos
[array([0.01113033, 0.30712296]), array([0.45228983, 0.10783232]), array([0.59659175, 0.31042038])]
BOUND: 0.048489625937646816


Good for bound 0.1; learned theta was 1.0
Map policy starts from (0.19156605232326396, 0.1980820738844118)
Good for bound 0.2; learned theta was 1.0
Map policy starts from (0.19156605232326396, 0.1980820738844118)
Good for bound 0.3; learned theta was 1.0
Map policy starts from (0.19156605232326396, 0.1980820738844118)
Good for bound 0.4; learned theta was 1.0
Map policy starts from (0.19156605232326396, 0.1980820738844118)
Good for bound 0.5; learned theta was 1.0
Map policy starts from (0.19156605232326396, 0.1980820738844118)
This demo is starting from 0.9516583034294708 0.3523472551118082
World 1 , 4 demos
[array([0.01113033, 0.30712296]), array([0.45228983, 0.10783232]), array([0.59659175, 0.31042038]), array([0.9516583 , 0.35234726])]
BOUND: 0.048489625937646816


Good for bound 0.1; learned theta was 1.0
Map policy starts from (0.19156605232326396, 0.1980820738844118)
Good for bound 0.2; learned theta was 1.0
Map policy starts from (0.19156605232326396, 0.1980820738844118)
Good for bound 0.3; learned theta was 1.0
Map policy starts from (0.19156605232326396, 0.1980820738844118)
Good for bound 0.4; learned theta was 1.0
Map policy starts from (0.19156605232326396, 0.1980820738844118)
Good for bound 0.5; learned theta was 1.0
Map policy starts from (0.19156605232326396, 0.1980820738844118)
This demo is starting from 0.889853959240609 0.8521416266817502
World 1 , 5 demos
[array([0.01113033, 0.30712296]), array([0.45228983, 0.10783232]), array([0.59659175, 0.31042038]), array([0.9516583 , 0.35234726]), array([0.88985396, 0.85214163])]
BOUND: 0.048489625937646816


Good for bound 0.1; learned theta was 1.0
Map policy starts from (0.19156605232326396, 0.1980820738844118)
Good for bound 0.2; learned theta was 1.0
Map policy starts from (0.19156605232326396, 0.1980820738844118)
Good for bound 0.3; learned theta was 1.0
Map policy starts from (0.19156605232326396, 0.1980820738844118)
Good for bound 0.4; learned theta was 1.0
Map policy starts from (0.19156605232326396, 0.1980820738844118)
Good for bound 0.5; learned theta was 1.0
Map policy starts from (0.19156605232326396, 0.1980820738844118)
NEW THRESHOLD 0.1
Num demos
1
2
3
4
5
Bound errors
0.048489625937646816
0.048489625937646816
0.048489625937646816
0.048489625937646816
0.048489625937646816
Policy optimalities
1.0
1.0
1.0
1.0
1.0
Accuracy
1.0
Confusion matrices
[[5, 0], [0, 0]]
NEW THRESHOLD 0.2
Num demos
1
2
3
4
5
Bound errors
0.048489625937646816
0.048489625937646816
0.048489625937646816
0.048489625937646816
0.048489625937646816
Policy optimalities
1.0
1.0
1.0
1.0
1.0
Accuracy
1.0
Confusion matrices
[[5, 0], [0, 0]]
NEW THRESHOLD 0.3
Num demos
1
2
3
4
5
Bound errors
0.048489625937646816
0.048489625937646816
0.048489625937646816
0.048489625937646816
0.048489625937646816
Policy optimalities
1.0
1.0
1.0
1.0
1.0
Accuracy
1.0
Confusion matrices
[[5, 0], [0, 0]]
NEW THRESHOLD 0.4
Num demos
1
2
3
4
5
Bound errors
0.048489625937646816
0.048489625937646816
0.048489625937646816
0.048489625937646816
0.048489625937646816
Policy optimalities
1.0
1.0
1.0
1.0
1.0
Accuracy
1.0
Confusion matrices
[[5, 0], [0, 0]]
NEW THRESHOLD 0.5
Num demos
1
2
3
4
5
Bound errors
0.048489625937646816
0.048489625937646816
0.048489625937646816
0.048489625937646816
0.048489625937646816
Policy optimalities
1.0
1.0
1.0
1.0
1.0
Accuracy
1.0
Confusion matrices
[[5, 0], [0, 0]]
**************************************************
