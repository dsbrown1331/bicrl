Environment
(0.5488993752471647, 0.510423098033572)
True reward function
1.0
True optimal policy
[[0.0, 0.0], [-0.03714081030094599, 0.11973146101020543], [-0.03269815040972511, 0.26703350345571036], [0.01805603837274742, 0.4337019547960835], [0.11839922328352927, 0.6076272963453642], [0.26738773850279257, 0.770718660712012], [0.4529637565784952, 0.8995353591863184], [0.6506597653572872, 0.980151659707909], [0.8380336937527416, 1.011925405594347], [1.0, 1.0]]
Num demos 1
EVDs
[0.051901161143220555, 0.035137288182053576, 0.020727897136163027, 0.009564094871471397, 0.002458779606747424, -0.0]
PMFs
[0.15929546652219861, 0.1622148137171831, 0.16515655560966, 0.16811997217523475, 0.17110432646881438, 0.1741088655069092]
Learned policies
[[0.0, 0.0], [-0.03714081030094599, 0.11973146101020543], [-0.03269815040972511, 0.26703350345571036], [0.01805603837274742, 0.4337019547960835], [0.11839922328352927, 0.6076272963453642], [0.26738773850279257, 0.770718660712012], [0.4529637565784952, 0.8995353591863184], [0.6506597653572872, 0.980151659707909], [0.8380336937527416, 1.011925405594347], [1.0, 1.0]]
Comparison grid
[[0.22222222222222224, 0.14326338732518015, 0.06430455242813807, -0.014654282468904017, -0.0936131173659461, -0.17257195226298813], [0.22428960009372317, 0.14137744830338306, 0.05846529651304294, -0.024446855277297175, -0.10735900706763729, -0.19027115885797735], [0.23233606181429275, 0.1441402626372644, 0.055944463460236044, -0.03225133571679237, -0.12044713489382067, -0.20864293407084902], [0.24898383211314565, 0.1541671901938136, 0.05935054827448158, -0.03546609364485051, -0.1302827355641825, -0.22509937748351452], [0.2765951745426339, 0.17390954083242655, 0.07122390712221921, -0.03146172658798818, -0.13414736029819546, -0.2368329940084028], [0.31601096153690605, 0.2045821128935354, 0.09315326425016474, -0.01827558439320598, -0.12970443303657658, -0.24113328167994724]]
**************************************************
