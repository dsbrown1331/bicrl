Ground truth theta: 1.0
Lava is at [0.54889938 0.5104231 ]
The optimal policies start from:
0.0 : (0.7334679331022486, 0.617957277378068)
0.2 : (0.3243521283770604, 0.48846307257692234)
0.4 : (0.6413571837168687, 0.22190746073126377)
0.6 : (0.15282405506912333, 0.11560150583077833)
0.8 : (0.4109942337204039, 0.9835208560033626)
1.0 : (0.39723511133476563, 0.4049261095286736)
This optimal policy starts from: (0.39723511133476563, 0.4049261095286736)
This demo is starting from 0.8854273358943412 0.9177146602595234
World 1 , 1 demos
[array([0.88542734, 0.91771466])]
BOUND: 0.04588404922607898


Good for bound 0.1; learned theta was 1.0
Map policy starts from (0.39723511133476563, 0.4049261095286736)
Good for bound 0.2; learned theta was 1.0
Map policy starts from (0.39723511133476563, 0.4049261095286736)
Good for bound 0.3; learned theta was 1.0
Map policy starts from (0.39723511133476563, 0.4049261095286736)
Good for bound 0.4; learned theta was 1.0
Map policy starts from (0.39723511133476563, 0.4049261095286736)
Good for bound 0.5; learned theta was 1.0
Map policy starts from (0.39723511133476563, 0.4049261095286736)
This demo is starting from 0.413719252229759 0.6983130547606924
World 1 , 2 demos
[array([0.88542734, 0.91771466]), array([0.41371925, 0.69831305])]
BOUND: 0.04588404922607898


Good for bound 0.1; learned theta was 1.0
Map policy starts from (0.39723511133476563, 0.4049261095286736)
Good for bound 0.2; learned theta was 1.0
Map policy starts from (0.39723511133476563, 0.4049261095286736)
Good for bound 0.3; learned theta was 1.0
Map policy starts from (0.39723511133476563, 0.4049261095286736)
Good for bound 0.4; learned theta was 1.0
Map policy starts from (0.39723511133476563, 0.4049261095286736)
Good for bound 0.5; learned theta was 1.0
Map policy starts from (0.39723511133476563, 0.4049261095286736)
This demo is starting from 0.2824800302815721 0.26675991671749477
World 1 , 3 demos
[array([0.88542734, 0.91771466]), array([0.41371925, 0.69831305]), array([0.28248003, 0.26675992])]
BOUND: 0.04588404922607898


Good for bound 0.1; learned theta was 1.0
Map policy starts from (0.39723511133476563, 0.4049261095286736)
Good for bound 0.2; learned theta was 1.0
Map policy starts from (0.39723511133476563, 0.4049261095286736)
Good for bound 0.3; learned theta was 1.0
Map policy starts from (0.39723511133476563, 0.4049261095286736)
Good for bound 0.4; learned theta was 1.0
Map policy starts from (0.39723511133476563, 0.4049261095286736)
Good for bound 0.5; learned theta was 1.0
Map policy starts from (0.39723511133476563, 0.4049261095286736)
This demo is starting from 0.8665329954795385 0.9823971891761035
World 1 , 4 demos
[array([0.88542734, 0.91771466]), array([0.41371925, 0.69831305]), array([0.28248003, 0.26675992]), array([0.866533  , 0.98239719])]
BOUND: 0.04588404922607898


Good for bound 0.1; learned theta was 1.0
Map policy starts from (0.39723511133476563, 0.4049261095286736)
Good for bound 0.2; learned theta was 1.0
Map policy starts from (0.39723511133476563, 0.4049261095286736)
Good for bound 0.3; learned theta was 1.0
Map policy starts from (0.39723511133476563, 0.4049261095286736)
Good for bound 0.4; learned theta was 1.0
Map policy starts from (0.39723511133476563, 0.4049261095286736)
Good for bound 0.5; learned theta was 1.0
Map policy starts from (0.39723511133476563, 0.4049261095286736)
This demo is starting from 0.21217300623005675 0.9084395033243936
World 1 , 5 demos
[array([0.88542734, 0.91771466]), array([0.41371925, 0.69831305]), array([0.28248003, 0.26675992]), array([0.866533  , 0.98239719]), array([0.21217301, 0.9084395 ])]
BOUND: 0.04588404922607898


Good for bound 0.1; learned theta was 1.0
Map policy starts from (0.39723511133476563, 0.4049261095286736)
Good for bound 0.2; learned theta was 1.0
Map policy starts from (0.39723511133476563, 0.4049261095286736)
Good for bound 0.3; learned theta was 1.0
Map policy starts from (0.39723511133476563, 0.4049261095286736)
Good for bound 0.4; learned theta was 1.0
Map policy starts from (0.39723511133476563, 0.4049261095286736)
Good for bound 0.5; learned theta was 1.0
Map policy starts from (0.39723511133476563, 0.4049261095286736)
NEW THRESHOLD 0.1
Num demos
1
2
3
4
5
Bound errors
0.04588404922607898
0.04588404922607898
0.04588404922607898
0.04588404922607898
0.04588404922607898
Policy optimalities
1.0
1.0
1.0
1.0
1.0
Accuracy
1.0
Confusion matrices
[[5, 0], [0, 0]]
NEW THRESHOLD 0.2
Num demos
1
2
3
4
5
Bound errors
0.04588404922607898
0.04588404922607898
0.04588404922607898
0.04588404922607898
0.04588404922607898
Policy optimalities
1.0
1.0
1.0
1.0
1.0
Accuracy
1.0
Confusion matrices
[[5, 0], [0, 0]]
NEW THRESHOLD 0.3
Num demos
1
2
3
4
5
Bound errors
0.04588404922607898
0.04588404922607898
0.04588404922607898
0.04588404922607898
0.04588404922607898
Policy optimalities
1.0
1.0
1.0
1.0
1.0
Accuracy
1.0
Confusion matrices
[[5, 0], [0, 0]]
NEW THRESHOLD 0.4
Num demos
1
2
3
4
5
Bound errors
0.04588404922607898
0.04588404922607898
0.04588404922607898
0.04588404922607898
0.04588404922607898
Policy optimalities
1.0
1.0
1.0
1.0
1.0
Accuracy
1.0
Confusion matrices
[[5, 0], [0, 0]]
NEW THRESHOLD 0.5
Num demos
1
2
3
4
5
Bound errors
0.04588404922607898
0.04588404922607898
0.04588404922607898
0.04588404922607898
0.04588404922607898
Policy optimalities
1.0
1.0
1.0
1.0
1.0
Accuracy
1.0
Confusion matrices
[[5, 0], [0, 0]]
**************************************************
